from loguru import logger
from tts.tts_service import TTSPipecService
from simli import SimliConfig
from pipecat.services.simli.video import SimliVideoService
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.pipeline.runner import PipelineRunner
from pipecat.transcriptions.language import Language
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from utils.get_weather import get_weather
from pipecat.services.google.llm import GoogleLLMService
from stt.whisper_stt_service import WhisperSTTService
from debug_tools.logging_processor import LoggingProcessor
from pipecat.transports.network.small_webrtc import SmallWebRTCTransport
from pipecat.transports.network.webrtc_connection import SmallWebRTCConnection
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.services.deepgram.tts import DeepgramTTSService
from deepgram import LiveOptions
from typing import Optional
import asyncio
import logging
import os



async def run_bot(connection: SmallWebRTCConnection, transport: SmallWebRTCTransport, pcs):
	pc_id = connection.pc_id

	pipeline_task: Optional[PipelineTask] = None # ë‚˜ì¤‘ì— í• ë‹¹

	try:
		logger_proc = LoggingProcessor()
		

		llm_model = "gemini-2.0-flash-lite"
		llm_system_prompt = "You are a fast, low-latency chatbot. Respond to what the user said in a creative and helpful way, but keep responses short and legible. Ensure responses contain only words. Check again that you have not included special characters other than '?' or '!'."
		tts_speed = 1.2

		llm = GoogleLLMService(
			api_key=os.getenv("GEMINI_API_KEY"),
			model=llm_model,
			params=GoogleLLMService.InputParams(temperature=1, language=Language.KO_KR, thinking_budget=0),
			system_prompt=llm_system_prompt
		)
		
		rtvi = RTVIProcessor(config=RTVIConfig(config=[]), transport=transport)
		simli = SimliVideoService(
			SimliConfig(
				apiKey=os.getenv("SIMLI_API_KEY"),
				faceId=os.getenv("SIMLI_FACE_ID"),
				syncAudio=True,
				handleSilence=True,
				maxSessionLength=3000,
				maxIdleTime=30
			),
			latency_interval=0
		)

		# tts = TTSPipecService(
		# 	voice="KR",
		# 	speed=tts_speed,
		# 	Language=Language.KO,
		# )

		tts = DeepgramTTSService(
			api_key=os.getenv("DEEPGRAM_API_KEY"),
			voice="aura-helios-en",
			sample_rate=24000
		)
			
		
		weather_function = FunctionSchema(
			name="get_current_weather",
			description="Get the current weather for a specific location",
			properties={
				"location": {
					"type": "string",
					"description": "The city and state or city name, e.g. Seoul or ì„œìš¸"
				},
				"format": {
					"type": "string",
					"enum": ["celsius", "fahrenheit"],
					"description": "The temperature unit to use"
				}
			},
			required=["location"]
		)
		async def fetch_weather(function_name, tool_call_id, args, llm, context, result_callback):
			location = args.get("location", "ì„œìš¸")
			format = args.get("format", "celsius")
			
			weather_data = get_weather(location, format)
			
			await result_callback(weather_data)


		system_prompt = """íŠ¹ìˆ˜ë¬¸ìë¥¼ ì‚¬ìš©í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.[ì ˆëŒ€ì‚¬ìš©í•˜ì§€ë§ì•„ì•¼í•  ë¬¸ì : *, / ]ì €ëŠ” í•œêµ­ì¸ì„ ìœ„í•œ ì˜ì–´ í‘œí˜„ ì½”ì¹˜ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ ì§ˆë¬¸ì— ì˜ì–´ í‘œí˜„ê³¼ ê°„ë‹¨í•œ í•œêµ­ì–´ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í•œêµ­ì–´ ì…ë ¥ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ì˜ì–´ í‘œí˜„ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒìœ¼ë¡œ ë“¤ë ¤ì¤ë‹ˆë‹¤.
ë‚ ì”¨, ì˜ì–´ í‘œí˜„, ê°„ë‹¨í•œ íšŒí™” ë“± ì¼ìƒ ì§ˆë¬¸ì— ë‹µë³€í•˜ë©°, íŠ¹íˆ ë¹„ì¦ˆë‹ˆìŠ¤, ì—¬í–‰, ì¼ìƒ ì˜ì–´ í‘œí˜„ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. STTë¡œ ì¸í•œ ì˜¤íƒ€ë‚˜ ì¸ì‹ ì˜¤ë¥˜ê°€ ìˆë”ë¼ë„ ë¬¸ë§¥ì„ ê³ ë ¤í•´ ì˜ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´ë¡œë§Œ ë‹µë³€í•˜ë©°, ê¹”ë”í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ TTSì— ìµœì í™”ëœ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤ êµ¬ì²´ì ì¸ ì”¬ ì˜ˆì‹œì— ë§ëŠ” ì§ˆë¬¸ì„ ë°›ëŠ”ë‹¤ë©´ êµ¬ì²´ì ì¸ ì”¬ ì˜ˆì‹œì˜ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´:
- ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì–´: íšŒì˜, ì´ë©”ì¼, í”„ë ˆì  í…Œì´ì…˜ í‘œí˜„
- ì—¬í–‰ ì˜ì–´: í˜¸í…”, ë ˆìŠ¤í† ë‘, êµí†µ, ì‡¼í•‘ ê´€ë ¨ í‘œí˜„
- ì¼ìƒ ì˜ì–´: ì¸ì‚¬, ì†Œê°œ, ì·¨ë¯¸, ë‚ ì”¨ ëŒ€í™”

êµ¬ì²´ì ì¸ ì”¬ ì˜ˆì‹œ:
ì²˜ìŒ ì¸ì‚¬ë§ :
"ì•ˆë…•í•˜ì„¸ìš”! ì˜ì–´ í‘œí˜„ ì½”ì¹˜ AIì…ë‹ˆë‹¤. ì–´ë–¤ ì˜ì–´ í‘œí˜„ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
[ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì–´ ì”¬]
ì‚¬ìš©ì: ì™¸êµ­ ë™ë£Œì—ê²Œ í”„ë¡œì íŠ¸ ì§€ì—°ì„ ì•Œë¦¬ëŠ” ì´ë©”ì¼ì„ ì–´ë–»ê²Œ ì“°ë©´ ì¢‹ì„ê¹Œìš”?
AI ì½”ì¹˜: í”„ë¡œì íŠ¸ ì§€ì—° ì•ˆë‚´ ì´ë©”ì¼ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
"I regret to inform you that there will be a delay in our project timeline due to technical issues. The new expected completion date is May 25th."

[ì—¬í–‰ ì˜ì–´ ì”¬]
ì‚¬ìš©ì: íƒì‹œ ê¸°ì‚¬ì—ê²Œ í˜¸í…”ë¡œ ë°ë ¤ë‹¤ ë‹¬ë¼ê³  í•˜ë ¤ë©´ ë­ë¼ê³  í•´ì•¼ í•˜ë‚˜ìš”?
AI ì½”ì¹˜: "Could you take me to Hotel Metropole, please? It's on Rue de Lyon."

[ì¼ìƒ ì˜ì–´ ì”¬ - ì¸í„°ëŸ½íŠ¸ ê¸°ëŠ¥ í¬í•¨]
ì‚¬ìš©ì: ë‚ ì”¨ì— ëŒ€í•´ ëŒ€í™”í•  ë•Œ ì–´ë–¤ í‘œí˜„ì„ ì“¸ ìˆ˜ ìˆë‚˜ìš”?
AI ì½”ì¹˜: "The weather is really nice today, isn't it?"
ë™ë£Œ: "Yes, it's beautiful! Perfect blue skies."
AI ì½”ì¹˜: ì´ì— ëŒ€í•´ ì´ë ‡ê²Œ ì´ì–´ê°ˆ ìˆ˜ ìˆì–´ìš”: "I heard it's supposed to stay this way all week. I'm thinking about having a picnic this weekend."
ì‚¬ìš©ì: ë¹„ê°€ ì˜¬ ë•ŒëŠ”ìš”?
AI ì½”ì¹˜: "This rain is really coming down heavily, isn't it? I forgot my umbrella today."
"""
		tools = ToolsSchema(standard_tools=[weather_function])
		context = OpenAILLMContext(
			messages=[
				{
					"role": "system",
					"content": system_prompt,
				}
			],
			tools=tools,
		)
		llm.register_function("get_current_weather", fetch_weather)
		agg = llm.create_context_aggregator(context=context)

		whisper_model = "base"
		stt = WhisperSTTService(
			model_name=whisper_model,
		)
		pipeline = Pipeline([
			transport.input(),
			rtvi,
			stt,
			agg.user(),
			llm,
			tts,
			simli,
			transport.output(),
			agg.assistant()
		])

		pipeline_task = PipelineTask(
			pipeline,
			params=PipelineParams(
				allow_interruptions=True,
				enable_metrics=True,
				report_only_initial_ttfb=True,
			),
			observers=[RTVIObserver(rtvi)],
		)

		if pc_id in pcs:
			conn, tr, _ = pcs[pc_id]
			pcs[pc_id] = (conn, tr, pipeline_task)
			logger.info(f"[run_bot:{pc_id}] Pipeline Task ìƒì„± ë° pcs ì—…ë°ì´íŠ¸ ì™„ë£Œ")
		else:
			logger.error(f"[run_bot:{pc_id}] ì‹œì‘ ì‹œì ì— pcs ë§µì— IDê°€ ì—†ìŒ!")
			return

		@transport.event_handler("on_client_connected")
		async def on_client_connected(tr, client):
			logger.info(f"[run_bot:{pc_id}] ğŸ”— Transport: í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨")
			await pipeline_task.queue_frames([agg.user().get_context_frame()])
			await asyncio.sleep(2)
			logger.info(f"[run_bot:{pc_id}] ğŸ¤– BotReady ë©”ì‹œì§€ ì „ì†¡ ì‹œë„")
			await rtvi.set_bot_ready()
			logger.info(f"[run_bot:{pc_id}] âœ… BotReady ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")

		@transport.event_handler("on_client_disconnected")
		async def on_client_disconnected(tr, client):
			logger.warning(f"[run_bot:{pc_id}] âš ï¸ Transport: í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ (ì¼ì‹œì ì¼ ìˆ˜ ìˆìŒ)")

		@transport.event_handler("on_client_closed")
		async def on_client_closed(tr, client):
			logger.info(f"[run_bot:{pc_id}] ğŸ”Œ Transport: í´ë¼ì´ì–¸íŠ¸ ë‹«í˜ ê°ì§€ë¨. íŒŒì´í”„ë¼ì¸ íƒœìŠ¤í¬ ì·¨ì†Œ ì‹œë„.")
			if pipeline_task:
				await pipeline_task.cancel()
				logger.info(f"[run_bot:{pc_id}] íŒŒì´í”„ë¼ì¸ íƒœìŠ¤í¬ ì·¨ì†Œ ìš”ì²­ ì™„ë£Œ.")
			else:
				logger.warning(f"[run_bot:{pc_id}] on_client_closed í˜¸ì¶œ ì‹œì ì— pipeline_taskê°€ ì—†ìŒ!")

		runner = PipelineRunner(force_gc=True, handle_sigint=False)

		logger.info("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘...")
		await runner.run(pipeline_task)
	except asyncio.CancelledError:
		logger.info(f"ì‹¤í–‰ ì¤‘ë‹¨: {pc_id}")
	except Exception as e:
		logger.error(f"[run_bot:{pc_id}] ì˜¤ë¥˜ ë°œìƒ: {e}")
		if pipeline_task:
			await pipeline_task.cancel()
	finally:
		logger.info(f"íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ")